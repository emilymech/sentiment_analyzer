{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "speaking-patio",
   "metadata": {},
   "source": [
    "Sentiment Analyzer - Source Code - Emily Mech\n",
    "\n",
    "This is the source code for the baseline and improved sentiment analyzers for the Computational Linguistics term project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "specified-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "egyptian-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self):\n",
    "        self.cwd = None\n",
    "        self.src = None\n",
    "        self.data = None\n",
    "        self.neg = None\n",
    "        self.pos = None\n",
    "        self.path_list = None\n",
    "        self.valence_path = None\n",
    "        self.df = None\n",
    "\n",
    "    def get_paths(self):\n",
    "        self.cwd = Path.cwd()\n",
    "        self.src = self.cwd.parent\n",
    "        self.neg = self.src / \"Data\" / \"review_polarity\" / \"txt_sentoken\" / \"neg\"\n",
    "        self.pos = self.src / \"Data\" / \"review_polarity\" / \"txt_sentoken\" / \"pos\"\n",
    "        self.path_list = [self.neg, self.pos]\n",
    "        self.valence_path = self.src / \"Data\" / \"NRC-Sentiment-Emotion-Lexicons\" / \"NRC-Sentiment-Emotion-Lexicons\" / \"NRC-VAD-Lexicon\" / \"NRC-VAD-Lexicon\" / \"v-scores.txt\" \n",
    "        \n",
    "    def read_data(self, path):\n",
    "        with open(path, \"r\") as f:\n",
    "            file = f.read()\n",
    "        return file\n",
    "    \n",
    "    def get_label(self, category):\n",
    "        if category.name == \"neg\":\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1 \n",
    "        return label\n",
    "    \n",
    "    def clean_tokens(self, file, stop=True, digit=True, punc=True, dash=True, stem=True):\n",
    "        tokens = file.split(' ')\n",
    "        tokens = [tok.strip('\\n') for tok in tokens]\n",
    "        tokens = [tok.lower() for tok in tokens]\n",
    "        \n",
    "        if dash:\n",
    "            tokens = [tok.strip('--') for tok in tokens]\n",
    "        if digit:     \n",
    "            tokens = [tok for tok in tokens if not tok.isnumeric()]\n",
    "        if punc:\n",
    "            punct = set(string.punctuation)\n",
    "            tokens = [tok for tok in tokens if tok not in punct]\n",
    "        if stop:\n",
    "            stop_words = set(stopwords.words('english')) \n",
    "            whitelist = ['not', \"n't\", \"no\"]\n",
    "            tokens = [w for w in tokens if (w not in stop_words or w in whitelist) and len(w) > 1] \n",
    "        if stem:\n",
    "            porter = PorterStemmer()\n",
    "            tokens = [porter.stem(word) for word in tokens]\n",
    "        \n",
    "        tokens = [tok for tok in tokens if tok]\n",
    "        join_tokens = ' '.join(tokens)\n",
    "        \n",
    "        return tokens, join_tokens    \n",
    "\n",
    "    def make_df(self, stop, digit, punc, dash, stem):\n",
    "        dict_list = []\n",
    "        header_list = [\"word\", \"rating\"]\n",
    "        v_scores = pd.read_csv(self.valence_path, delimiter = \"\\t\", names = header_list)\n",
    "        for path_category in self.path_list:\n",
    "            label = self.get_label(path_category)\n",
    "            files = path_category.glob(\"**/*\")\n",
    "            for file in files:\n",
    "                text_label_dict = {}\n",
    "                f = self.read_data(file)\n",
    "                clean_tokens, joined_clean_tokens = self.clean_tokens(f, stop, digit, punc, dash, stem)\n",
    "                token_ratings = v_scores.loc[v_scores['word'].isin(clean_tokens)] \n",
    "                pos_count = len(token_ratings[token_ratings['rating'] > .5].sum(axis=1))\n",
    "                neg_count = len(token_ratings[token_ratings['rating'] < .5].sum(axis=1))\n",
    "                avg_rating = token_ratings[\"rating\"].mean()\n",
    "                text_label_dict[\"label\"] = label\n",
    "                text_label_dict[\"text\"] = f\n",
    "                text_label_dict[\"clean_text\"] = joined_clean_tokens\n",
    "                text_label_dict[\"pos_count\"] = str(pos_count)\n",
    "                text_label_dict[\"neg_count\"] = str(neg_count)\n",
    "                text_label_dict[\"avg_rating\"] = str(avg_rating)\n",
    "                dict_list.append(text_label_dict)\n",
    "            self.df = pd.DataFrame(dict_list)\n",
    "            \n",
    "            \n",
    "class Classify(Preprocess):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        Preprocess.get_paths(self)\n",
    "        Preprocess.make_df(self, stop=True, digit=True, punc=True, dash=True, stem=True)  # to modify text representations, change any of the desired arguments to False\n",
    "        self.output_dict_list = []\n",
    "        \n",
    "    def vectorize_input(self, preprocess_type, vectorizer_type, feature_type):\n",
    "        if vectorizer_type == \"tfidf\":\n",
    "            vect = TfidfVectorizer()\n",
    "        elif vectorizer_type == \"count\":\n",
    "            vect = CountVectorizer()\n",
    "            \n",
    "        if preprocess_type == \"raw\":\n",
    "            col_name = \"text\"\n",
    "        elif preprocess_type == \"cleaned\":\n",
    "            col_name = \"clean_text\"\n",
    "        \n",
    "        if feature_type == \"improved\":\n",
    "            column_trans = ColumnTransformer([('positive', CountVectorizer(lowercase=False),'pos_count'),\n",
    "                                              ('negative', CountVectorizer(lowercase=False), 'neg_count'), \n",
    "                                              ('average', CountVectorizer(lowercase=False), 'avg_rating'),\n",
    "                                              ('text', vect, col_name)], remainder='drop')\n",
    "            \n",
    "            features = column_trans.fit_transform(self.df)\n",
    "        \n",
    "        elif feature_type == \"intermediate\":\n",
    "            column_trans = ColumnTransformer([('positive', CountVectorizer(lowercase=False),'pos_count'),\n",
    "                                              ('negative', CountVectorizer(lowercase=False), 'neg_count'),\n",
    "                                              ('text', vect, col_name)], remainder='drop')\n",
    "            \n",
    "            features = column_trans.fit_transform(self.df)\n",
    "            \n",
    "        else:\n",
    "            features = vect.fit_transform(self.df[col_name])\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def buildClassifiers(self, clf, X_train, X_test, y_train, y_test, features):\n",
    "        clf.fit(X_train.todense(), y_train)\n",
    "        y_pred = clf.predict(X_test.todense())\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "        precision = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        return f1, precision, recall, accuracy\n",
    "    \n",
    "    # Construct the classifiers at hand prior to folding the data through them\n",
    "    def run_classifiers(self):\n",
    "        text_types = ['raw', 'cleaned']\n",
    "        vect_types = ['tfidf', 'count']\n",
    "        feature_types = ['baseline', 'improved', 'intermediate']\n",
    "        \n",
    "        names = ['Logistic_Regression', 'Decision_Tree', \"Neural_Network\"]\n",
    "        classifiers = [LogisticRegression(random_state=0, max_iter=1000),\n",
    "                       DecisionTreeClassifier(random_state=0),\n",
    "                       MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=1000)]\n",
    "        \n",
    "        for text_type in text_types:\n",
    "            for vect_type in vect_types:\n",
    "                for feature_type in feature_types:\n",
    "                    for name, clf in zip(names, classifiers):\n",
    "                        output_dict = {}\n",
    "                        features = self.vectorize_input(text_type, vect_type, feature_type)\n",
    "                        f1_list = []\n",
    "                        precision_list = []\n",
    "                        recall_list = []\n",
    "                        accuracy_list = []\n",
    "\n",
    "                        print(\"Now classifying...\")\n",
    "                        print(f\"classifier:{name}, text:{text_type}, vect:{vect_type}, feature:{feature_type}\")\n",
    "\n",
    "                        # Fold the data 5 times\n",
    "                        kf = KFold(n_splits = 5)\n",
    "                        foldCounter = 0\n",
    "\n",
    "                        for train_index, test_index in kf.split(features):\n",
    "                            X_train, X_test = features[train_index], features[test_index]\n",
    "                            y_train, y_test = self.df['label'][train_index], self.df['label'][test_index]\n",
    "\n",
    "                            f1, precision, recall, accuracy = self.buildClassifiers(clf, X_train, X_test, y_train, y_test, features)\n",
    "                            f1_list.append(f1)\n",
    "                            precision_list.append(precision)\n",
    "                            recall_list.append(recall)\n",
    "                            accuracy_list.append(accuracy)\n",
    "\n",
    "                        mean_f1 = np.mean(f1_list)\n",
    "                        mean_precision = np.mean(precision_list)\n",
    "                        mean_recall = np.mean(recall_list)\n",
    "                        mean_accuracy = np.mean(accuracy_list)\n",
    "                        \n",
    "                        print(\"\\tAverage Macro F1 for {}:\\t\\t\".format(name), mean_f1)\n",
    "                        print(\"\\tAverage Macro Precision for {}:\\t\".format(name), mean_precision)\n",
    "                        print(\"\\tAverage Macro Recall for {}:\\t\\t\".format(name), mean_recall)\n",
    "\n",
    "                        output_dict['clf'] = name\n",
    "                        output_dict['text'] = text_type\n",
    "                        output_dict['vect'] = vect_type\n",
    "                        output_dict['feature'] = feature_type\n",
    "                        output_dict['f1'] = mean_f1\n",
    "                        output_dict['precision'] = mean_precision\n",
    "                        output_dict['recall'] = mean_recall\n",
    "                        output_dict['accuracy'] = mean_accuracy\n",
    "                        self.output_dict_list.append(output_dict)\n",
    "        \n",
    "        return self.output_dict_list\n",
    "                 \n",
    "    def save_output(self, output_dict_list):\n",
    "        output_df = pd.DataFrame(output_dict_list)\n",
    "        result_path = self.src / \"Results\" / \"results.csv\"\n",
    "        output_df.to_csv(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-saturday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
